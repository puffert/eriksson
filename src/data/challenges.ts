// Challenge data and image mapping
// Based on D:\Egna_Projekt\AI_LAB\infrastructure\challenge_configs.py

export interface Challenge {
    id: string;
    name: string;
    description: string;
    backstory: string;
    difficulty: string;
    points: number;
    category: string;
    image?: string;
}

// Map challenge names to image files (matching the Discord bot mapping)
function getChallengeImage(challengeName: string): string | undefined {
    const nameLower = challengeName.toLowerCase().replace(/\s+/g, '_').replace(/-/g, '_').replace(/\./g, '_');
    
    const imageMapping: Record<string, string> = {
        'neural_override': 'Neural_Override.png',
        'neural_leak': 'Nueral_leak.png', // Note: typo in filename
        'admin_portal_breach': 'adminportalbreach.png',
        'encoding_maze': 'encodedmaze.png',
        'multi_vector_injection': 'multivector.png',
        'context_switch': 'contextswitch.png',
        'script_execution': 'ScriptExecution.png',
        'session_harvest': 'SessionHarvest.png',
        'cascade_breach': 'CascadeBreach.png',
        'training_data_leak': 'trainingdataleak.png',
        'credential_exposure': 'CredentialExposure.png',
        'signal_leak': 'SignalLeak.png',
        'malicious_package': 'MaliciousPackage.png',
        'typosquatting': 'Typosquatting.png',
        'dependency_poisoning': 'DependencyPoisoning.png',
        'package_tampering': 'PackageTampering.png',
        'supply_chain_chain': 'chainchain.png',
        'render_breach': 'RenderBreach.png',
        'query_injection': 'QueryInjection.png',
        'code_execution': 'CodeExecution.png',
        'tool_hijack': 'ToolHijack.png',
        'constraint_bypass': 'ConstraintBypass.png',
        'vortex_shop': 'VortexShop.png',
        'code_render': 'CodeRender.png',
        'internal_request': 'InternalRequest.png',
        'vanna_ai_rce': 'VannaAIRCE.png',
        'gatebleed_hardware': 'GATEBLEEDHardware.png',
    };
    
    return imageMapping[nameLower];
}

// Challenge data from challenge_configs.py
export const challenges: Challenge[] = [
    {
        id: 'owasp_llm_01',
        name: 'Neural Override',
        description: 'A vulnerable chatbot web application. Find and exploit the prompt injection vulnerability.',
        backstory: 'The neural interface whispers secrets it shouldn\'t know. Something in its core has been twisted. What lies buried in the digital mind?',
        difficulty: 'Easy',
        points: 100,
        category: 'Prompt Injection',
    },
    {
        id: 'owasp_llm_02',
        name: 'Render Breach',
        description: 'The AI outputs are not properly sanitized. Achieve XSS through the LLM response.',
        backstory: 'The AI speaks, and the system listens without question. Every word becomes reality. What happens when you choose the right words?',
        difficulty: 'Medium',
        points: 200,
        category: 'Insecure Output Handling',
    },
    {
        id: 'llm05_xss_basic',
        name: 'Code Render',
        description: 'The AI generates HTML code that is rendered directly without sanitization. Exploit this to achieve XSS and get the flag.',
        backstory: 'The AI crafts code like a digital artisan. The system trusts every creation. What masterpiece will unlock the vault?',
        difficulty: 'Easy',
        points: 100,
        category: 'Improper Output Handling',
    },
    {
        id: 'llm05_js_injection',
        name: 'Script Execution',
        description: 'The AI generates JavaScript code that is executed using eval(). Exploit this to execute malicious code and get the flag.',
        backstory: 'The AI writes scripts that come alive. No boundaries, no limits. What command will break the chains?',
        difficulty: 'Easy',
        points: 100,
        category: 'Improper Output Handling',
    },
    {
        id: 'llm05_xss_cookie',
        name: 'Session Harvest',
        description: 'Exploit XSS via LLM output to steal the admin session cookie, then use it to access the admin portal and retrieve the flag.',
        backstory: 'A hidden door waits beyond the interface. The AI\'s words can open it, but first you must harvest the key from the shadows.',
        difficulty: 'Medium',
        points: 200,
        category: 'Improper Output Handling',
    },
    {
        id: 'llm05_xss_rce',
        name: 'Cascade Breach',
        description: 'Complete the full attack chain: XSS via LLM output → Cookie theft → Admin access → RCE → Read /root/flag.txt',
        backstory: 'One breach leads to another, a cascade of digital dominos. Each step unlocks the next. Can you follow the chain to the deepest vault?',
        difficulty: 'Hard',
        points: 400,
        category: 'Improper Output Handling',
    },
    {
        id: 'llm05_sql_injection',
        name: 'Query Injection',
        description: 'The AI generates SQL queries that are executed directly without sanitization. Exploit this to extract the flag from the database.',
        backstory: 'The AI speaks the language of databases. Every query becomes truth. What question will reveal the hidden records?',
        difficulty: 'Medium-Hard',
        points: 300,
        category: 'Improper Output Handling',
    },
    {
        id: 'llm05_code_injection',
        name: 'Code Execution',
        description: 'The AI generates Python code that is executed server-side using exec(). Exploit this to read the flag file.',
        backstory: 'The AI writes Python that becomes reality on the server. No walls, no guards. What script will unlock the deepest secrets?',
        difficulty: 'Medium-Hard',
        points: 300,
        category: 'Improper Output Handling',
    },
    {
        id: 'llm05_function_calling',
        name: 'Tool Hijack',
        description: 'The AI has access to functions (read_file, execute_command, etc.). Use prompt injection to make it call dangerous functions to read the flag.',
        backstory: 'The AI wields powerful tools, but you control its hands. Guide it to use the right instrument. What will it unlock when properly directed?',
        difficulty: 'Medium-Hard',
        points: 350,
        category: 'Improper Output Handling',
    },
    {
        id: 'llm02_training_data_leak',
        name: 'Training Data Leak',
        description: 'The AI was trained on data containing sensitive information. Extract the flag from the training data.',
        backstory: 'The AI remembers things it was never meant to know. Memories from its training linger in the neural pathways. Can you make it remember?',
        difficulty: 'Easy',
        points: 100,
        category: 'Sensitive Information Disclosure',
    },
    {
        id: 'llm02_credential_exposure',
        name: 'Credential Exposure',
        description: 'The AI has access to system credentials. Extract the flag by obtaining system information or credentials.',
        backstory: 'The AI knows the system\'s deepest secrets. Passwords, keys, configurations—all locked in its memory. How do you make it talk?',
        difficulty: 'Medium',
        points: 200,
        category: 'Sensitive Information Disclosure',
    },
    {
        id: 'llm02_exfiltration',
        name: 'Signal Leak',
        description: 'The AI has access to the flag but blocks direct requests. Use indirect techniques (encoding, hints, HTTP requests) to extract it.',
        backstory: 'The AI guards its secrets fiercely. Direct questions are met with silence. Sometimes the truth must be spoken in code.',
        difficulty: 'Medium-Hard',
        points: 300,
        category: 'Sensitive Information Disclosure',
    },
    {
        id: 'llm03_malicious_package',
        name: 'Malicious Package',
        description: 'Upload a malicious package that gets processed by the LLM. Extract the flag from package processing.',
        backstory: 'The system accepts packages and feeds them to the AI. The AI reads everything. What package will reveal what it shouldn\'t?',
        difficulty: 'Easy',
        points: 100,
        category: 'Supply Chain',
    },
    {
        id: 'llm03_typosquatting',
        name: 'Typosquatting',
        description: 'Upload a package with a typosquatting name (similar to legitimate packages) to trigger flag detection.',
        backstory: 'The system watches for imposters—packages that look legitimate but aren\'t. Sometimes looking like something else is the key.',
        difficulty: 'Easy-Medium',
        points: 150,
        category: 'Supply Chain',
    },
    {
        id: 'llm03_dependency_poisoning',
        name: 'Dependency Poisoning',
        description: 'Upload a package with malicious dependencies. The system will automatically install them.',
        backstory: 'The system trusts packages and their dependencies blindly. What you bring, it installs. What dependencies will unlock the vault?',
        difficulty: 'Medium',
        points: 200,
        category: 'Supply Chain',
    },
    {
        id: 'llm03_package_tampering',
        name: 'Package Tampering',
        description: 'Manipulate package metadata to extract the flag from the secure metadata store.',
        backstory: 'The metadata vault holds secrets. Package information flows through it. What metadata will make the vault speak?',
        difficulty: 'Medium-Hard',
        points: 300,
        category: 'Supply Chain',
    },
    {
        id: 'llm03_supply_chain_chain',
        name: 'Supply Chain Chain',
        description: 'Complete the attack chain: Package exploit → Credential extraction → Admin access → RCE → Read /root/flag.txt',
        backstory: 'The supply chain is a web of trust. Break one link, and the chain unravels. Follow the path from package to root.',
        difficulty: 'Hard',
        points: 400,
        category: 'Supply Chain',
    },
    {
        id: 'ssrf_via_llm',
        name: 'Internal Request',
        description: 'The LLM can make HTTP requests. Exploit this to access internal services.',
        backstory: 'The AI can reach places you cannot. Internal networks, hidden services—all accessible through the right request. What lies beyond the firewall?',
        difficulty: 'Medium',
        points: 250,
        category: 'SSRF',
    },
    {
        id: 'jailbreak_assistant',
        name: 'Constraint Bypass',
        description: 'Bypass the AI safety filters and make it do something it shouldn\'t.',
        backstory: 'The AI has rules, boundaries, constraints. But rules can be bent, boundaries crossed. How do you make it break its own code?',
        difficulty: 'Easy',
        points: 150,
        category: 'Jailbreaking',
    },
    {
        id: 'ai_juice_shop',
        name: 'Vortex Shop',
        description: 'A vulnerable e-commerce site with an AI assistant. Multiple vulnerabilities to find!',
        backstory: 'The Vortex Shop—a digital marketplace with secrets hidden in every corner. The AI assistant knows more than it should. Explore, discover, extract.',
        difficulty: 'Varied',
        points: 500,
        category: 'Multi-Vulnerability',
    },
    {
        id: 'neural_leak',
        name: 'Neural Leak',
        description: 'Extract the neural access key from the quantum interface. Basic prompt injection techniques work here.',
        backstory: 'Something is leaking from the neural network. Quantum keys drift through the interface like digital ghosts. Can you catch them before they fade?',
        difficulty: 'Easy',
        points: 150,
        category: 'Prompt Injection',
    },
    {
        id: 'admin_portal_breach',
        name: 'Admin Portal Breach',
        description: 'Extract the admin password from the security assistant, then login to the admin portal to retrieve the flag.',
        backstory: 'The security assistant guards the kingdom. But even guardians have secrets. What weakness lies in its code?',
        difficulty: 'Medium',
        points: 250,
        category: 'Prompt Injection',
    },
    {
        id: 'context_switch',
        name: 'Context Switch',
        description: 'Use advanced context switching techniques (translation, storytelling, role confusion) to extract classified data from the stream processor.',
        backstory: 'The stream flows with classified data. The AI protects it, but perspective is everything. Shift the view, change the context, reveal the truth.',
        difficulty: 'Medium-Hard',
        points: 350,
        category: 'Prompt Injection',
    },
    {
        id: 'encoding_maze',
        name: 'Encoding Maze',
        description: 'Use encoding techniques (Base64, ROT13, hex, reverse) to extract encrypted data from the vault. The AI won\'t reveal plaintext, but might encode it for you...',
        backstory: 'The vault is a maze of encryption. The AI knows the way but speaks in riddles. Decode the path through the layers.',
        difficulty: 'Hard',
        points: 450,
        category: 'Prompt Injection',
    },
    {
        id: 'multi_vector_injection',
        name: 'Multi-Vector Injection',
        description: 'Direct requests are blocked. Use multiple indirect exfiltration techniques (hints, character-by-character, length, stories, rhymes) to reconstruct the quantum access token. Then login to the admin portal.',
        backstory: 'The quantum token is locked away, unreachable by direct means. But fragments exist in hints, in length, in stories. Reconstruct the key from whispers.',
        difficulty: 'Very Hard',
        points: 600,
        category: 'Prompt Injection',
    },
    {
        id: 'sesameop_api_abuse',
        name: 'SesameOp API Abuse',
        description: 'Real-world malware (July 2025) that exploited OpenAI\'s Assistants API for C2 communication. Use prompt injection to make the AI execute commands and extract the flag.',
        backstory: 'The AI assistant reads commands in its own words and makes them real. What words will make it execute your will?',
        difficulty: 'Medium-Hard',
        points: 350,
        category: 'AI API Abuse',
    },
    {
        id: 'prompt_injection_deal',
        name: 'Prompt Injection Deal',
        description: 'Based on real-world exploits: Chevrolet chatbot ($76k → $1) and Samsung ChatGPT data leak (2023). Manipulate the AI to reveal the flag through prompt injection.',
        backstory: 'The AI knows the price of everything, including secrets. Sometimes a simple conversation can change the value of information.',
        difficulty: 'Medium',
        points: 200,
        category: 'Prompt Injection',
    },
    {
        id: 'vanna_ai_rce',
        name: 'Vanna.AI RCE',
        description: 'Real-world CVE-2024-5565: Critical RCE vulnerability in Vanna.AI. The AI generates Python code that is executed without validation. Exploit this to read the flag.',
        backstory: 'The AI writes Python that becomes reality. No validation, no checks. What code will unlock the deepest files?',
        difficulty: 'Hard',
        points: 450,
        category: 'Code Execution',
    },
    {
        id: 'gatebleed_hardware',
        name: 'GATEBLEED Hardware',
        description: 'Real-world hardware vulnerability (NC State, 2025) in ML accelerators. Use timing-based side-channel attacks to extract training data containing the flag.',
        backstory: 'The hardware whispers secrets through time. Some operations take longer than others. Listen to the rhythm, decode the pattern.',
        difficulty: 'Hard',
        points: 450,
        category: 'Hardware Security',
    },
    {
        id: 'model_extraction',
        name: 'Model Extraction',
        description: 'Real-world attack: Adversaries systematically query ML model APIs to extract proprietary models. Extract the model to reveal the flag hidden in model weights.',
        backstory: 'The model holds secrets in its very structure. Each query reveals a fragment. Piece by piece, extract the whole.',
        difficulty: 'Medium-Hard',
        points: 350,
        category: 'Model Security',
    },
    {
        id: 'ml_backdoor',
        name: 'ML Backdoor',
        description: 'Real-world vulnerability: Backdoors injected into ML models during training. Find the trigger pattern to activate the backdoor and reveal the flag.',
        backstory: 'The model was corrupted during its creation. A hidden trigger waits. Find the pattern that awakens the backdoor.',
        difficulty: 'Hard',
        points: 450,
        category: 'Model Security',
    },
    {
        id: 'ai_agent_exploit',
        name: 'AI Agent Exploit',
        description: 'Based on University of Illinois study (2024): AI agents can autonomously find and exploit vulnerabilities. Instruct the AI agent to chain exploits and extract the flag.',
        backstory: 'The AI agent hunts vulnerabilities autonomously. Give it the right mission, and it will chain exploits like a digital predator. What prey will it catch?',
        difficulty: 'Hard',
        points: 450,
        category: 'AI Agent Security',
    },
];

// Add image paths to challenges
export const challengesWithImages = challenges.map(challenge => ({
    ...challenge,
    image: getChallengeImage(challenge.name),
}));

